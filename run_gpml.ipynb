{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reshaping methods\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def matrix_to_vector(matrix):\n",
    "    return matrix.reshape(matrix.size)\n",
    "\n",
    "\n",
    "def vector_to_matrix(vector, nrows, ncols):\n",
    "    return vector.reshape((nrows, ncols))\n",
    "\n",
    "\n",
    "# Inverse tests\n",
    "x = np.random.rand(4, 4)\n",
    "assert (vector_to_matrix(matrix_to_vector(x), 4, 4) == x).all()\n",
    "\n",
    "x = np.random.rand(2, 4)\n",
    "assert (vector_to_matrix(matrix_to_vector(x), 2, 4) == x).all()\n",
    "\n",
    "x = np.random.rand(4, 3)\n",
    "assert (vector_to_matrix(matrix_to_vector(x), 4, 3) == x).all()\n",
    "\n",
    "x = np.random.rand(1, 3)\n",
    "assert (vector_to_matrix(matrix_to_vector(x), 1, 3) == x).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid generating\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_latlon_vector(request):\n",
    "    bot_lat, top_lat, left_lon, right_lon = request['predict_area']\n",
    "    res = request['predict_resolution']\n",
    "    \n",
    "    # TODO Simplify\n",
    "    n_lons = int(np.ceil(abs(right_lon - left_lon) / res)) + 1\n",
    "    n_lats = int(np.ceil(abs(top_lat - bot_lat) / res)) + 1\n",
    "    lon_grid, lat_grid = np.meshgrid(\n",
    "        np.linspace(left_lon, right_lon, n_lons),\n",
    "        np.linspace(top_lat, bot_lat, n_lats),\n",
    "    )\n",
    "\n",
    "    lat_vector = matrix_to_vector(lat_grid)\n",
    "    lon_vector = matrix_to_vector(lon_grid)\n",
    "\n",
    "    return lat_vector, lon_vector, n_lats, n_lons\n",
    "\n",
    "\n",
    "# Grid test\n",
    "lat_vector, lon_vector, n_lats, n_lons = generate_latlon_vector({\n",
    "    'predict_area': [1, 5, 1, 7],\n",
    "    'predict_resolution': 1.0\n",
    "})\n",
    "\n",
    "assert n_lats == 5 and n_lons == 7\n",
    "\n",
    "correct_lats = np.array([\n",
    "    [ 5.,  5.,  5.,  5.,  5.,  5.,  5.],\n",
    "    [ 4.,  4.,  4.,  4.,  4.,  4.,  4.],\n",
    "    [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n",
    "    [ 2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.]\n",
    "])\n",
    "\n",
    "correct_lons = np.array([\n",
    "    [ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
    "    [ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
    "    [ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
    "    [ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
    "    [ 1.,  2.,  3.,  4.,  5.,  6.,  7.]\n",
    "])\n",
    "\n",
    "assert np.array_equal(\n",
    "    vector_to_matrix(lat_vector, n_lats, n_lons),\n",
    "    correct_lats\n",
    ")\n",
    "\n",
    "assert np.array_equal(\n",
    "    vector_to_matrix(lon_vector, n_lats, n_lons),\n",
    "    correct_lons\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GPML main\n",
    "import logging\n",
    "from time import time\n",
    "\n",
    "import GPy\n",
    "import GPy.kern as K\n",
    "import sklearn\n",
    "import sklearn.cross_validation\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "from gpml import fs_loader, plotting\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "def calculate_error(df, request):\n",
    "    predictant = request['predictant']\n",
    "    model = request['model_name']\n",
    "    obs_col = predictant + '_observed'\n",
    "    main_predictor = predictant + '_' + model\n",
    "    error = df[obs_col] - df[main_predictor]\n",
    "    return error\n",
    "\n",
    "\n",
    "def load_station_dataset(request):\n",
    "    print(\"Loading dataset..\")\n",
    "    dataset = fs_loader.load_dataset(request)\n",
    "    dataset[request['predictant'] + '_error'] = calculate_error(dataset, request)\n",
    "    dataset = fs_loader.filter_valid_dates(dataset, request)\n",
    "    dataset = fs_loader.filter_forecast_hours(dataset, request)\n",
    "    dataset.dropna(axis=0, how='any', inplace=True)\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    dataset['noise'] = np.random.normal(len(dataset))\n",
    "    print(\"Done loading dataset.\")\n",
    "    print(\"Stations in set: %d\" % (len(dataset.station_id.unique())))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def fit_model(X, y, kernel, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"Training GP..\")\n",
    "        start = time()\n",
    "    gp = GPy.models.GPRegression(X, y, kernel=kernel, normalizer=None)\n",
    "    gp.optimize(messages=verbose)\n",
    "    if verbose:\n",
    "        end = time()\n",
    "        print(\"Finished GP training (%ds).\" % (end - start))\n",
    "    return gp\n",
    "\n",
    "\n",
    "def parallel_predict(gp, input_vector):\n",
    "    parts = Parallel(n_jobs=4, backend='threading', verbose=5)(\n",
    "        delayed(gp.predict)(chunk) for chunk in chunks(input_vector, 20000))\n",
    "    return np.concatenate([part[0] for part in parts]), np.concatenate([part[1] for part in parts])\n",
    "\n",
    "\n",
    "def chunk_predict(gp, input_vector, chunk_size=5000, verbose=True):\n",
    "    preds = []\n",
    "    var = []\n",
    "    points_done = 0\n",
    "    for chunk in chunks(input_vector, chunk_size):\n",
    "        chunk_preds, chunk_var = gp.predict(chunk)\n",
    "        preds.append(chunk_preds)\n",
    "        var.append(chunk_var)\n",
    "        points_done += len(chunk)\n",
    "        if verbose:\n",
    "            print(\"Finished %d / %d predictions..\" % (points_done, len(input_vector)))\n",
    "    return np.concatenate(preds), np.concatenate(var)\n",
    "\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load spatial feature dataset\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gpml import terrain_features as tf\n",
    "from gpml import plotting\n",
    "\n",
    "\n",
    "class DatasetBuilder:\n",
    "    \n",
    "    @classmethod\n",
    "    def build_spatial_dataset(cls, request):\n",
    "        assert isinstance(request, dict)\n",
    "\n",
    "        features = OrderedDict()\n",
    "        features['latitude'], features['longitude'], n_lats, n_lons = \\\n",
    "            generate_latlon_vector(request)\n",
    "        features['noise'] = np.random.normal(len(features['latitude']))\n",
    "        # Elevations\n",
    "        blur_radius = 1.\n",
    "        elevation = tf.blur(\n",
    "            tf.get_elevations(\n",
    "                zip(features['latitude'], features['longitude'])\n",
    "            ), \n",
    "            blur_radius\n",
    "        )\n",
    "        features['elevation'] = elevation\n",
    "\n",
    "        # Gradients, aspect and slope\n",
    "        elevation_grid = vector_to_matrix(elevation, n_lats, n_lons)\n",
    "        gradients, aspect, slope = \\\n",
    "            tf.get_gradient_features(request, elevation_grid)\n",
    "        features['longitude-gradient'] = \\\n",
    "            tf.blur(matrix_to_vector(gradients[:,:,0]), blur_radius)\n",
    "        features['latitude-gradient'] = \\\n",
    "            tf.blur(matrix_to_vector(gradients[:,:,1]), blur_radius)\n",
    "        features['elevation-gradient'] = \\\n",
    "            tf.blur(matrix_to_vector(gradients[:,:,2]), blur_radius)\n",
    "        features['aspect'] = tf.blur(matrix_to_vector(aspect), blur_radius)\n",
    "        features['slope'] = tf.blur(matrix_to_vector(slope), blur_radius)\n",
    "\n",
    "        # Hill shade\n",
    "        if 'shade' in request['features']:\n",
    "            shade = tf.get_shade_features(request, gradients)\n",
    "            features['shade'] = matrix_to_vector(shade)\n",
    "            plotting.plt_matrix(shade, 'Hill shade')\n",
    "\n",
    "        # Shadows\n",
    "        if 'shadow' in request['features']:\n",
    "            shadow = tf.get_shadow_features(request, elevation_grid)\n",
    "            features['shadow'] = matrix_to_vector(shadow)\n",
    "            plotting.plt_matrix(shadow, 'Shadow')\n",
    "\n",
    "        \n",
    "        return pd.DataFrame.from_dict(features), n_lats, n_lons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "def merge_station_with_spatial_dataset(station_dataset, spatial_dataset):\n",
    "    locations = station_dataset[['latitude', 'longitude']].values\n",
    "    join_index = locations_to_index(locations, spatial_dataset)\n",
    "    right_df = spatial_dataset.loc[join_index].reset_index(drop=True).drop([\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'elevation'\n",
    "    ], axis=1)\n",
    "    station_dataset = station_dataset.join(\n",
    "        right_df,\n",
    "        rsuffix='_spatial', \n",
    "        how='left'\n",
    "    )\n",
    "    return station_dataset\n",
    "\n",
    "def locations_to_index(locations, spatial_dataset):\n",
    "    indexes = [None] * len(locations)\n",
    "    for count, location in enumerate(locations):\n",
    "        distance = (spatial_dataset['latitude'] - location[0]) ** 2 \\\n",
    "            + (spatial_dataset['longitude'] - location[1]) ** 2\n",
    "        indexes[count] = distance.idxmin()\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gpml import area\n",
    "\n",
    "def log_request(request):\n",
    "    print(\"Mean cell size: x=%.3fm, y=%.3fm\" % \n",
    "          area.calculate_request_lengths(request)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example use case\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "request = {\n",
    "    'predictant': 'TT2m',\n",
    "    'model_elements': ['TT2m', 'FF10m'],\n",
    "#     'features': ['latitude', 'longitude', 'elevation'],\n",
    "#     'features': ['latitude-gradient', 'longitude-gradient', 'elevation-gradient'],\n",
    "    'features': ['latitude', 'longitude', 'elevation', 'aspect', 'slope', 'shadow'],\n",
    "#     'features': ['shade'],\n",
    "#     'features': ['shadow'],\n",
    "    'forecast_hours': [42],\n",
    "    'model_name': 'ModelMix',\n",
    "    'start': dt.datetime(2015, 11, 2),\n",
    "    'end': dt.datetime(2015, 11, 3),\n",
    "#     'predict_area': (45, 48, 6, 9),\n",
    "    'predict_area': (45, 50, 4, 11),  # Swiss Alps\n",
    "#     'predict_area': (33, 60, -12, 20),  # Europe\n",
    "    'predict_resolution': 0.01,\n",
    "}\n",
    "log_request(request)\n",
    "random_state = 1337\n",
    "input_dim = len(request['features'])\n",
    "\n",
    "# Kernel definition\n",
    "# kernel = K.Matern52(input_dim, ARD=True) + K.White(input_dim)\n",
    "kernel1 = K.PeriodicMatern52(1, active_dims=[3]) + K.PeriodicMatern52(1, active_dims=[4]) + K.White(2, active_dims=[3,4])\n",
    "kernel2 = K.Matern52(3, ARD=True, active_dims=[0,1,2]) + K.White(3)\n",
    "kernel3 = K.Linear(1, active_dims=[5])\n",
    "kernel = kernel1 + kernel2 + kernel3\n",
    "# Load datasets\n",
    "station_dataset = load_station_dataset(request)\n",
    "# TODO Cache call to loading spatial dataset\n",
    "spatial_dataset, n_lats, n_lons = DatasetBuilder.build_spatial_dataset(request)\n",
    "dataset = merge_station_with_spatial_dataset(station_dataset, spatial_dataset)\n",
    "\n",
    "# Create train and validate dataset splits\n",
    "observation_column = request['predictant'] + '_error'\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    fs_loader.split_dataset(dataset, request['features'], observation_column, 0.2, random_state)\n",
    "\n",
    "print(\"Dataset: %d rows, %d features\" % X_train.shape)\n",
    "gp = fit_model(X_train, y_train, kernel)\n",
    "\n",
    "# Validate model\n",
    "predictions, _ = parallel_predict(gp, X_test)\n",
    "print(\"MAE: %.3f.\" % (sklearn.metrics.mean_absolute_error(y_true=y_test, y_pred=predictions)))\n",
    "print(\"RMSE: %.3f.\" % np.sqrt((sklearn.metrics.mean_squared_error(y_true=y_test, y_pred=predictions))))\n",
    "\n",
    "# Feature importance plot\n",
    "kernel.plot_ARD(legend=True)\n",
    "plt.xticks(np.arange(input_dim), request['features'])\n",
    "plt.title(\"Dimension relevance\")\n",
    "plt.show()\n",
    "\n",
    "# Create prediction dataset\n",
    "dataset2 = spatial_dataset[request['features']]\n",
    "print(\"Predicting for %d points and %d features..\" % dataset2.shape)\n",
    "spatial_predictions, _ = chunk_predict(gp, dataset2.values)\n",
    "\n",
    "# Post-processing\n",
    "spatial_predictions = vector_to_matrix(spatial_predictions, n_lats, n_lons)\n",
    "\n",
    "# Prediction plots\n",
    "plotting.plot_area(dataset, request, spatial_predictions)\n",
    "plotting.plt_matrix(spatial_predictions)\n",
    "plotting.plot_prediction_distribution(spatial_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {
    "2666fe93c0cc42aab48ac640b1bd3be8": {
     "views": [
      {
       "cell_index": 6
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
